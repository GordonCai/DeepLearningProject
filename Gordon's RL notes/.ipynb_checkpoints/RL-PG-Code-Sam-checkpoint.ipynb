{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Solves Pong with Policy Gradients in Tensorflow.'''\n",
    "# written October 2016 by Sam Greydanus\n",
    "# inspired by karpathy's gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5\n",
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "\n",
    "# hyperparameters\n",
    "n_obs = 80 * 80           # dimensionality of observations\n",
    "h = 200                   # number of hidden layer neurons\n",
    "n_actions = 3             # number of available actions\n",
    "learning_rate = 1e-3\n",
    "gamma = .99               # discount factor for reward\n",
    "decay = 0.99              # decay rate for RMSProp gradients\n",
    "save_path='models-TF/pong.ckpt'\n",
    "\n",
    "# gamespace \n",
    "env = gym.make(\"Pong-v0\") # environment info\n",
    "observation = env.reset()\n",
    "prev_x = None\n",
    "xs,rs,ys = [],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0\n",
    "\n",
    "# initialize model\n",
    "tf_model = {}\n",
    "with tf.variable_scope('layer_one',reuse=False):\n",
    "    xavier_l1 = tf.truncated_normal_initializer(mean=0, stddev=1./np.sqrt(n_obs), dtype=tf.float32)\n",
    "    tf_model['W1'] = tf.get_variable(\"W1\", [n_obs, h], initializer=xavier_l1)\n",
    "with tf.variable_scope('layer_two',reuse=False):\n",
    "    xavier_l2 = tf.truncated_normal_initializer(mean=0, stddev=1./np.sqrt(h), dtype=tf.float32)\n",
    "    tf_model['W2'] = tf.get_variable(\"W2\", [h,n_actions], initializer=xavier_l2)\n",
    "\n",
    "# tf operations\n",
    "def tf_discount_rewards(tf_r): #tf_r ~ [game_steps,1]\n",
    "    discount_f = lambda a, v: a*gamma + v;\n",
    "    tf_r_reverse = tf.scan(discount_f, tf.reverse(tf_r,[True, False]))\n",
    "    tf_discounted_r = tf.reverse(tf_r_reverse,[True, False])\n",
    "    return tf_discounted_r\n",
    "\n",
    "def tf_policy_forward(x): #x ~ [1,D]\n",
    "    h = tf.matmul(x, tf_model['W1'])\n",
    "    h = tf.nn.relu(h)\n",
    "    logp = tf.matmul(h, tf_model['W2'])\n",
    "    p = tf.nn.softmax(logp)\n",
    "    return p\n",
    "\n",
    "# downsampling\n",
    "def prepro(I):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    I = I[35:195] # crop\n",
    "    I = I[::2,::2,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0  # erase background (background type 1)\n",
    "    I[I == 109] = 0  # erase background (background type 2)\n",
    "    I[I != 0] = 1    # everything else (paddles, ball) just set to 1\n",
    "    return I.astype(np.float).ravel()\n",
    "\n",
    "# tf placeholders\n",
    "tf_x = tf.placeholder(dtype=tf.float32, shape=[None, n_obs],name=\"tf_x\")\n",
    "tf_y = tf.placeholder(dtype=tf.float32, shape=[None, n_actions],name=\"tf_y\")\n",
    "tf_epr = tf.placeholder(dtype=tf.float32, shape=[None,1], name=\"tf_epr\")\n",
    "\n",
    "# tf reward processing (need tf_discounted_epr for policy gradient wizardry)\n",
    "tf_discounted_epr = tf_discount_rewards(tf_epr)\n",
    "tf_mean, tf_variance= tf.nn.moments(tf_discounted_epr, [0], shift=None, name=\"reward_moments\")\n",
    "tf_discounted_epr -= tf_mean\n",
    "tf_discounted_epr /= tf.sqrt(tf_variance + 1e-6)\n",
    "\n",
    "# tf optimizer op\n",
    "tf_aprob = tf_policy_forward(tf_x)\n",
    "loss = tf.nn.l2_loss(tf_y-tf_aprob)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate, decay=decay)\n",
    "tf_grads = optimizer.compute_gradients(loss, var_list=tf.trainable_variables(), grad_loss=tf_discounted_epr)\n",
    "train_op = optimizer.apply_gradients(tf_grads)\n",
    "\n",
    "# tf graph initialization\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
    "tf.initialize_all_variables().run()\n",
    "\n",
    "# try load saved model\n",
    "saver = tf.train.Saver(tf.all_variables())\n",
    "load_was_success = True # yes, I'm being optimistic\n",
    "try:\n",
    "    save_dir = '/'.join(save_path.split('/')[:-1])\n",
    "    ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "    load_path = ckpt.model_checkpoint_path\n",
    "    saver.restore(sess, load_path)\n",
    "except:\n",
    "    print(\"no saved model to load. starting new session\")\n",
    "    load_was_success = False\n",
    "else:\n",
    "    print(\"loaded model: {}\".format(load_path))\n",
    "    saver = tf.train.Saver(tf.all_variables())\n",
    "    episode_number = int(load_path.split('-')[-1])\n",
    "\n",
    "\n",
    "# training loop\n",
    "while True:\n",
    "#     if True: env.render()\n",
    "\n",
    "    # preprocess the observation, set input to network to be difference image\n",
    "    cur_x = prepro(observation)\n",
    "    x = cur_x - prev_x if prev_x is not None else np.zeros(n_obs)\n",
    "    prev_x = cur_x\n",
    "\n",
    "    # stochastically sample a policy from the network\n",
    "    feed = {tf_x: np.reshape(x, (1,-1))}\n",
    "    aprob = sess.run(tf_aprob,feed) ; aprob = aprob[0,:]\n",
    "    action = np.random.choice(n_actions, p=aprob)\n",
    "    label = np.zeros_like(aprob) ; label[action] = 1\n",
    "\n",
    "    # step the environment and get new measurements\n",
    "    observation, reward, done, info = env.step(action+1)\n",
    "    reward_sum += reward\n",
    "    \n",
    "    # record game history\n",
    "    xs.append(x) ; ys.append(label) ; rs.append(reward)\n",
    "    \n",
    "    if done:\n",
    "        # update running reward\n",
    "        running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "        \n",
    "        # parameter update\n",
    "        feed = {tf_x: np.vstack(xs), tf_epr: np.vstack(rs), tf_y: np.vstack(ys)}\n",
    "        _ = sess.run(train_op,feed)\n",
    "        \n",
    "        # print progress console\n",
    "        if episode_number % 10 == 0:\n",
    "            print('ep {}: reward: {}, mean reward: {:3f}'.format(episode_number, reward_sum, running_reward))\n",
    "        else:\n",
    "            print('\\tep {}: reward: {}'.format(episode_number, reward_sum))\n",
    "        \n",
    "        # bookkeeping\n",
    "        xs,rs,ys = [],[],[] # reset game history\n",
    "        episode_number += 1 # the Next Episode\n",
    "        observation = env.reset() # reset env\n",
    "        reward_sum = 0\n",
    "        if episode_number % 50 == 0:\n",
    "            saver.save(sess, save_path, global_step=episode_number)\n",
    "            print(\"SAVED MODEL #{}\".format(episode_number))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
