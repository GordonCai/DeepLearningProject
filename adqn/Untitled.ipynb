{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "import collections as deque \n",
    "from skimage import color \n",
    "from skimage import color, transform, exposure\n",
    "import pickle \n",
    "import warnings\n",
    "import gym \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_observation(self, observation): \n",
    "    x_t1 = color.rgb2gray(observation)\n",
    "    x_t1 = skimage.transform.resize(x_t1, (84,84))\n",
    "    x_t1 = skimage.exposure.rescale_intensity(x_t1, out_range=(0,255))\n",
    "    x_t1 = np.uint8(x_t1)\n",
    "        #x_t1 = x_t1.reshape(1,1, x_t1.shape[0], x_t1.shape[1])\n",
    "    return x_t1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DQNAgent(): \n",
    "    def __init__(self, resized_height, resized_width, action_size, session, batch_size):\n",
    "        tf.global_step = tf.Variable(0, name='Frame', trainable=False) \n",
    "        self.frame_inc_op = self.global_step.assign_add(1, use_locking=True)\n",
    "        self.session = session\n",
    "\n",
    "\n",
    "        with tf.variable_scope('network'):\n",
    "            self.action_size = action_size \n",
    "            self.state = tf.placeholder(dtype=float32, shape=[None, 84, 84, 1], name='state')\n",
    "            self.action = tf.placeholder(dtype=int32, shape=[None], name='action')\n",
    "            self.y = tf.placeholder(dtype=int32, shape=[None], name='y')\n",
    "            self.reward = tf.placeholder(dtype=float32, shape=[None], name='reward')\n",
    "        \n",
    "            self.gamma = 0.9\n",
    "            self.epsilon = 1.0 \n",
    "            self.e_decay = 0.99 \n",
    "            self.e_min = 0.01 \n",
    "            self.learning_rate = 0.01 \n",
    "            self.total_reward = 0.0\n",
    "            self.total_loss = 0.0 \n",
    "    \n",
    "            conv1 = tf.contrib.layers.conv2d(self.state, 32, 8, 8, activation_fn=tf.nn.relu)\n",
    "            conv2 = tf.contrib.layers.conv2d(conv1, 64, 4, 4, activation_fn=tf.nn.relu)\n",
    "            conv3 = tf.contrib.layers.conv2d(conv2, 64, 3, 3, activation_fn=tf.nn.relu)\n",
    "            flattened = tf.contrib.layers.flatten(conv3)\n",
    "            fc1 = tf.contrib.layers.fully_connected(flattened, 512, activation_fn=tf.nn.relu)\n",
    "            self.prediction = tf.contrib.layers.fully_connected(fc1, self.action_size)\n",
    "        with tf.variable_scope('optimizer'):\n",
    "            action_ind = tf.range(batch_sizes) * tf.shape(self.prediction)[1] + self.action\n",
    "            self.Q = tf.gather(tf.reshape(self.prediction, [-1]), action_ind)\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "            self.loss = tf.reduce_mean(tf.squared_difference(self.y, self.Q))\n",
    "            self.trainable_weights = tf.get_trainable_weights()\n",
    "            grads = tf.gradients(self.loss, self.trainable_weights)\n",
    "            grads, _ = tf.clip_by_global_norm(grads, 40)\n",
    "            grad_vars = list(zip(grads, self.weights))\n",
    "            self.train_op = self.optimizer.apply_gradients(grad_vars)\n",
    "    def frame(self): \n",
    "        return self.global_step.eval(session=self.sess)\n",
    "    def update_target(self): \n",
    "        self.sess.run(self.target_update)\n",
    "    def predict_rewards(self, state): \n",
    "        return self.sess.run(self.q_values, {self.state, state}).flatten()\n",
    "    def predict_target(self, state): \n",
    "        return np.max(self.sess.run(self.target_q_values, {self.target_state: state}).flatten())\n",
    "    def train(self, states, actions, rewards): \n",
    "        self.sess.run(self.train_op, feed_dict={self.state: state, self.action: action, self.reward: rewards})\n",
    "    def frame_increment(self): \n",
    "        self.frame_inc_op_eval(session=self.sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Cite \n",
    "class AgentSummary:\n",
    "    \"\"\"Helper wrapper for summary tensorboard logging\"\"\"\n",
    "    def __init__(self, logdir, agent, env_name):\n",
    "        \"\"\" :param logdir: path to the log directory\n",
    "            :param agent: agent class-wrapper\n",
    "            :param env_name: environment name\"\"\"\n",
    "        with tf.variable_scope('summary'):\n",
    "            self.agent = agent\n",
    "            self.last_time = time.time()\n",
    "            self.last_frames = self.agent.frame\n",
    "            scalar_tags = ['fps', 'episode_avg_reward', 'avg_q_value',\n",
    "                           'epsilon', 'total_frame_step']\n",
    "            self.writer = tf.train.SummaryWriter(logdir, self.agent.sess.graph)\n",
    "            self.summary_vars = {}\n",
    "            self.summary_ph = {}\n",
    "            self.summary_ops = {}\n",
    "            for k in scalar_tags:\n",
    "                self.summary_vars[k] = tf.Variable(0.)\n",
    "                self.summary_ph[k] = tf.placeholder('float32', name=k)\n",
    "                self.summary_ops[k] = tf.scalar_summary(\"%s/%s\" % (env_name, k), self.summary_vars[k])\n",
    "            self.update_ops = []\n",
    "            for k in self.summary_vars:\n",
    "                self.update_ops.append(self.summary_vars[k].assign(self.summary_ph[k]))\n",
    "            self.summary_op = tf.merge_summary(list(self.summary_ops.values()))\n",
    "\n",
    "    def write_summary(self, tags):\n",
    "        tags['fps'] = (self.agent.frame - self.last_frames) / (time.time() - self.last_time)\n",
    "        self.last_time = time.time()\n",
    "        self.last_frames = self.agent.frame\n",
    "        self.agent.sess.run(self.update_ops, {self.summary_ph[k]: v for k, v in tags.items()})\n",
    "        summary = self.agent.sess.run(self.summary_op, \n",
    "                                      {self.summary_vars[k]: v for k, v in tags.items()})\n",
    "        self.writer.add_summary(summary, global_step=self.agent.frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
