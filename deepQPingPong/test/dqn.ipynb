{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym \n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "from PIL import Image \n",
    "from collections import deque\n",
    "import random \n",
    "# from keras import initializers\n",
    "# from keras.initializers import normal, identity\n",
    "# from keras.models import model_from_json\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "# from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "# from keras.optimizers import SGD , Adam, RMSprop\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import skimage\n",
    "from skimage import color \n",
    "from skimage import color, transform, exposure\n",
    "import argparse\n",
    "from collections import deque\n",
    "import warnings\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPISODES = 20000\n",
    "CONFIG = 'nothreshold'\n",
    " # number of valid actions\n",
    "GAMMA = 0.99 # decay rate of past observations\n",
    "OBSERVATION = 3200. # timesteps to observe before training\n",
    "EXPLORE = 3000000. # frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
    "INITIAL_EPSILON = 0.1 # starting value of epsilon\n",
    "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
    "BATCH = 32 # size of minibatch\n",
    "FRAME_PER_ACTION = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "MAX_MEMORY = 100000\n",
    "batch_sizes = 32\n",
    "rewardList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_observation(self, observation): \n",
    "    x_t1 = color.rgb2gray(observation)\n",
    "    x_t1 = skimage.transform.resize(x_t1, (80,80))\n",
    "    x_t1 = skimage.exposure.rescale_intensity(x_t1, out_range=(0,255))\n",
    "    x_t1 = np.uint8(x_t1)\n",
    "        #x_t1 = x_t1.reshape(1,1, x_t1.shape[0], x_t1.shape[1])\n",
    "    return x_t1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQNAgent(): \n",
    "    def __init__(self, state_size, action_size):\n",
    "        with tf.device('/cpu:0'):\n",
    "            \n",
    "            self.state_size = state_size\n",
    "            self.action_size = action_size \n",
    "            self.memory = deque(maxlen=100000)\n",
    "            self.gamma = 0.9 \n",
    "            self.epsilon = 1.0 \n",
    "            self.e_decay = 0.99 \n",
    "            self.e_min = 0.05 \n",
    "            self.learning_rate = 0.01 \n",
    "            self.total_reward = 0.0\n",
    "            self.total_loss = 0.0 \n",
    "            self.y = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
    "\n",
    "            self.state = tf.placeholder(tf.float32, [None, 80, 80, 4], name=\"state\")\n",
    "            self.action = tf.placeholder(tf.int32, shape=[None], name=\"actions\")\n",
    "            \n",
    "            backToState = tf.to_float(self.state) / 255.0\n",
    "            conv1 = tf.contrib.layers.conv2d(self.state, 32, 8, 8, activation_fn=tf.nn.relu)\n",
    "            conv2 = tf.contrib.layers.conv2d(conv1, 64, 4, 4, activation_fn=tf.nn.relu)\n",
    "            conv3 = tf.contrib.layers.conv2d(conv2, 64, 3, 3, activation_fn=tf.nn.relu)\n",
    "            flattened = tf.contrib.layers.flatten(conv3)\n",
    "            fc1 = tf.contrib.layers.fully_connected(flattened, 512, activation_fn=tf.nn.relu)\n",
    "            self.prediction = tf.contrib.layers.fully_connected(fc1, self.action_size)\n",
    "            action_ind = tf.range(batch_sizes) * tf.shape(self.prediction)[1] + self.action\n",
    "            self.Q = tf.gather(tf.reshape(self.prediction, [-1]), action_ind)\n",
    "            print(self.Q)\n",
    "            # mse loss \n",
    "            self.loss = tf.reduce_mean(tf.squared_difference(self.y, self.Q))\n",
    "            self.optimizer =  tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "            self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        reward = np.clip(reward, -1.0, 1.0)\n",
    "        self.total_reward += reward\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if(len(self.memory) > MAX_MEMORY): \n",
    "            self.memory.popleft()\n",
    "            \n",
    "    def act(self, sess, state): \n",
    "        if np.random.rand() <= self.epsilon: \n",
    "            return random.randrange(self.action_size)\n",
    "        else: \n",
    "            state = state.reshape(1, state.shape[0], state.shape[1],state.shape[2])\n",
    "\n",
    "            q = sess.run([self.prediction], {self.state: state})\n",
    "            action = np.argmax(q)\n",
    "            return action \n",
    "\n",
    "    def predict(self, sess, state):\n",
    "            return sess.run([self.prediction], {self.state: state})\n",
    "\n",
    "    def update(self, sess, state, action, y):\n",
    "        feed_dict = {self.state: state, self.y: y, self.action: action}\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict)\n",
    "        return loss\n",
    "    \n",
    "    def replay(self, sess, batch_size): \n",
    "        batch_size = min(batch_size, len(self.memory))\n",
    "        \n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        state_batch = []\n",
    "        action_batch = []\n",
    "        reward_batch = []\n",
    "        next_state_batch = []\n",
    "        terminal_batch = []\n",
    "        y_batch = []\n",
    "        \n",
    "        for i in range(batch_size): \n",
    "            state, action, reward, next_state, done = minibatch[i]\n",
    "            \n",
    "            if done: \n",
    "                terminal_batch.append(0)\n",
    "            else: \n",
    "                terminal_batch.append(1)\n",
    "                \n",
    "            next_state_batch.append(next_state)\n",
    "            action_batch.append(action)\n",
    "            reward_batch.append(reward)\n",
    "            state_batch.append(state) \n",
    "        terminal_batch = np.array(terminal_batch) + 0\n",
    "\n",
    "        target_values = self.predict(sess, next_state_batch)[0]\n",
    "        y_batch = reward_batch + self.gamma * (1 - terminal_batch) * np.max(target_values, axis=1)\n",
    "\n",
    "        loss = self.update(sess, state_batch, action_batch, y_batch)\n",
    "        self.total_loss += loss\n",
    "        \n",
    "        if self.epsilon > self.e_min: \n",
    "            self.epsilon *= self.e_decay \n",
    "            \n",
    "    def return_loss():\n",
    "        return self.total_loss\n",
    "    \n",
    "    def return_reward(): \n",
    "        return self.total_reward\n",
    "      \n",
    "    def preprocess_observation(self, observation): \n",
    "        x_t1 = color.rgb2gray(observation)\n",
    "        x_t1 = skimage.transform.resize(x_t1, (80,80))\n",
    "        x_t1 = skimage.exposure.rescale_intensity(x_t1, out_range=(0,255))\n",
    "        x_t1 = np.uint8(x_t1)\n",
    "        return x_t1 \n",
    "     \n",
    "  \n",
    "    def initial_state(self, state):\n",
    "        x_t = self.preprocess_observation(state)\n",
    "        s_t = np.stack((x_t, x_t, x_t, x_t), axis=2) \n",
    "        s_t = s_t.reshape(s_t.shape[0], s_t.shape[1],s_t.shape[2])\n",
    "        return s_t\n",
    "    \n",
    "    def set_reward(self, reward):\n",
    "        self.total_reward = reward\n",
    "        \n",
    "    def load(self, name): \n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, name)\n",
    "\n",
    "    def save(self, name): \n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(tf.get_default_session(), name)\n",
    "\n",
    "    def set_loss(self):\n",
    "        self.total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-26 13:28:35,741] Making new env: Pong-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Gather:0\", shape=(32,), dtype=float32, device=/device:CPU:0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f69c29a0335d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-f69c29a0335d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mjhaveri/anaconda3/envs/DeepLearning/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mjhaveri/anaconda3/envs/DeepLearning/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mjhaveri/anaconda3/envs/DeepLearning/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mjhaveri/anaconda3/envs/DeepLearning/lib/python3.6/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSimpleImageViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_action_meanings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mjhaveri/anaconda3/envs/DeepLearning/lib/python3.6/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, arr)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"You passed in an image with the wrong number shape\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'RGB'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpitch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mjhaveri/anaconda3/envs/DeepLearning/lib/python3.6/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36mclear\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mThe\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0mmust\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mactive\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m         '''\n\u001b[0;32m-> 1150\u001b[0;31m         \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglClear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGL_COLOR_BUFFER_BIT\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGL_DEPTH_BUFFER_BIT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdispatch_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mjhaveri/anaconda3/envs/DeepLearning/lib/python3.6/site-packages/pyglet/gl/lib.py\u001b[0m in \u001b[0;36merrcheck\u001b[0;34m(result, func, arguments)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0merrcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_debug_gl_trace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    env = gym.make('Pong-v0')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n \n",
    "    state = 0 \n",
    "    g = tf.Graph()\n",
    "    config = tf.ConfigProto(allow_soft_placement = True)\n",
    "\n",
    "    with g.as_default(), tf.Session(config=config) as sess, tf.device('/cpu:0'):\n",
    "        agent = DQNAgent(state_size, action_size)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for e in range(EPISODES):     \n",
    "        #state = agent.preprocess_observation(state)          \n",
    "            state = env.reset() \n",
    "            state = agent.initial_state(state)\n",
    "            agent.set_reward(0)\n",
    "            agent.set_loss()\n",
    "        #state = agent.preprocess_observation(state)\n",
    "            done = False \n",
    "            while not done: \n",
    "                env.render() \n",
    "                action = agent.act(sess, state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                next_state = agent.preprocess_observation(next_state) \n",
    "                next_state = next_state.reshape(next_state.shape[0], next_state.shape[1], 1)\n",
    "                next_state = np.append(next_state, state[:,:,:3], axis=2)\n",
    "                reward = reward if not done else -10 \n",
    "                agent.remember(state, action, reward, next_state, done)\n",
    "                state = next_state \n",
    "                if (e > 0):\n",
    "                    agent.replay(sess, 32)\n",
    "                    if done: \n",
    "                        output += \"\\n\" + \"episode: {}/{}, score: {}, loss: {}\".format(e, EPISODES, agent.total_reward, agent.loss)\n",
    "                        rewardList.append(agent.total_reward)\n",
    "\n",
    "                        if (e%1 == 0):\n",
    "                            agent.save('model1')\n",
    "                            with open(\"test.txt\", \"a\") as myfile:\n",
    "                                myfile.write(output)\n",
    "                            pickle.dump(rewardList, open('rewards.p', 'wb'))\n",
    "                            output = \" \"\n",
    "                        break\n",
    "                    #print(sumReward)         \n",
    "                else: \n",
    "                    if done:\n",
    "                        output = \"\\n\" \"episode: {}/{}, score: {}, loss: {}\".format(e, EPISODES, agent.total_reward, agent.loss)\n",
    "                        with open(\"test.txt\", \"a\") as myfile:\n",
    "                            myfile.write(output)\n",
    "                        rewardList.append(agent.total_reward)\n",
    "                        pickle.dump(rewardList, open('rewards.p', 'wb'))\n",
    "                        print(output)\n",
    "                        break \n",
    "            \n",
    "        \n",
    "\n",
    "    env.close()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
