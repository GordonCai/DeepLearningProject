{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import keras \n",
    "from PIL import Image \n",
    "from collections import deque\n",
    "import random \n",
    "from keras import initializers\n",
    "from keras.initializers import normal, identity\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD , Adam, RMSprop\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import skimage\n",
    "from skimage import color \n",
    "from skimage import color, transform, exposure\n",
    "import argparse\n",
    "from collections import deque\n",
    "import warnings\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPISODES = 20000\n",
    "CONFIG = 'nothreshold'\n",
    " # number of valid actions\n",
    "GAMMA = 0.99 # decay rate of past observations\n",
    "OBSERVATION = 3200. # timesteps to observe before training\n",
    "EXPLORE = 3000000. # frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
    "INITIAL_EPSILON = 0.1 # starting value of epsilon\n",
    "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
    "BATCH = 32 # size of minibatch\n",
    "FRAME_PER_ACTION = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "MAX_MEMORY = 100000\n",
    "rewardList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent: \n",
    "    def __init__(self, state_size, action_size): \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size \n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.gamma = 0.9 \n",
    "        self.epsilon = 1.0 \n",
    "        self.e_decay = 0.99 \n",
    "        self.e_min = 0.05 \n",
    "        self.learning_rate = 0.01 \n",
    "        self.model = self._build_model()\n",
    "        self.loss = 0.0\n",
    "        self.total_reward = 0.0\n",
    "    def _build_model(self): \n",
    "        with tf.device('cpu:0'):\n",
    "            model = Sequential()\n",
    "            model.add(Convolution2D(32, 8, 8, subsample=(4, 4), border_mode='same',input_shape=(80,80,4)))  #80*80*4\n",
    "            model.add(Activation('relu'))\n",
    "            model.add(Convolution2D(64, 4, 4, subsample=(2, 2), border_mode='same'))\n",
    "            model.add(Activation('relu'))\n",
    "            model.add(Convolution2D(64, 3, 3, subsample=(1, 1), border_mode='same'))\n",
    "            model.add(Activation('relu'))\n",
    "            model.add(Flatten())\n",
    "            model.add(Dense(512))\n",
    "            model.add(Activation('relu'))\n",
    "            model.add(Dense(self.action_size))\n",
    "   \n",
    "            adam = Adam(lr=LEARNING_RATE)\n",
    "            model.compile(loss='mse',optimizer=adam)\n",
    "        return model \n",
    "    \n",
    "    def set_loss(self): \n",
    "        self.loss = 0\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        reward = np.clip(reward, -1.0, 1.0)\n",
    "        self.total_reward += reward\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if(len(self.memory) > MAX_MEMORY): \n",
    "            self.memory.popleft()\n",
    "    \n",
    "    def act(self, state): \n",
    "        if np.random.rand() <= self.epsilon: \n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "    \n",
    "    def replay(self, batch_size): \n",
    "        batch_size = min(batch_size, len(self.memory))\n",
    "        \n",
    "        minibatch = random.sample(self.memory, batch_size) \n",
    "        X = np.zeros((batch_size, 80, 80, 4))\n",
    "        Y = np.zeros((batch_size, self.action_size)) \n",
    "\n",
    "        for i in range(batch_size): \n",
    "            state, action, reward, next_state, done = minibatch[i]\n",
    "            Q_sa = self.model.predict(state)\n",
    "            target = reward\n",
    "            #self.total_reward += reward\n",
    "            if done: \n",
    "                target = reward  \n",
    "            else: \n",
    "                target = reward + self.gamma * np.max(Q_sa)\n",
    "            X[i], Y[i] = state, target\n",
    "            #print(\"target \", target.shape)\n",
    "            #print(\"targetf \",Y[i,:])\n",
    "        history = self.model.fit(X, Y, batch_size=batch_size, nb_epoch=1, verbose=0)\n",
    "        self.loss += history.history['loss'][0]\n",
    "        if self.epsilon > self.e_min: \n",
    "            self.epsilon *= self.e_decay \n",
    "            \n",
    "    def preprocess_observation(self, observation): \n",
    "        x_t1 = color.rgb2gray(observation)\n",
    "        x_t1 = skimage.transform.resize(x_t1, (80,80))\n",
    "        x_t1 = skimage.exposure.rescale_intensity(x_t1, out_range=(0,255))\n",
    "        x_t1 = np.uint8(x_t1)\n",
    "        #x_t1 = x_t1.reshape(1,1, x_t1.shape[0], x_t1.shape[1])\n",
    "        return x_t1 \n",
    "    \n",
    "    def initial_state(self, state):\n",
    "        x_t = self.preprocess_observation(state)\n",
    "        s_t = np.stack((x_t, x_t, x_t, x_t), axis=2) \n",
    "        s_t = s_t.reshape(1,  s_t.shape[0], s_t.shape[1],s_t.shape[2])\n",
    "        return s_t\n",
    "    \n",
    "    def set_reward(self, reward):\n",
    "        self.total_reward = reward\n",
    "        \n",
    "    def load(self, name): \n",
    "        self.model.save_weights(name)\n",
    "\n",
    "    def save(self, name): \n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-26 03:54:44,957] Making new env: Pong-v0\n"
     ]
    }
   ],
   "source": [
    "with tf.device('gpu:0'):\n",
    "\n",
    "    env = gym.make('Pong-v0')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n \n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    #state = env.reset() \n",
    "    output = []\n",
    "    for e in range(EPISODES):     \n",
    "        #state = agent.preprocess_observation(state)  \n",
    "        \n",
    "        sumReward = 0 \n",
    "        \n",
    "        state = env.reset() \n",
    "        state = agent.initial_state(state)\n",
    "        agent.set_reward(0)\n",
    "        agent.set_loss()\n",
    "        #state = agent.preprocess_observation(state)\n",
    "        done = False \n",
    "        while not done: \n",
    "            env.render() \n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = agent.preprocess_observation(next_state) \n",
    "            next_state = next_state.reshape(1, next_state.shape[0], next_state.shape[1], 1)\n",
    "            next_state = np.append(next_state, state[:,:,:,:3], axis=3)\n",
    "\n",
    "            reward = reward if not done else -10 \n",
    "            sumReward += reward\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state \n",
    "            if (e > 0):\n",
    "                agent.replay(32)\n",
    "                if done: \n",
    "                    output += \"\\n\" + \"episode: {}/{}, score: {}, loss: {}\".format(e, EPISODES, agent.total_reward, agent.loss)\n",
    "                    rewardList.append(agent.total_reward)\n",
    "\n",
    "                    if (e%100 == 0):\n",
    "                        agent.save('model.h5')\n",
    "                        with open(\"test.txt\", \"a\") as myfile:\n",
    "                            myfile.write(output)\n",
    "                        pickle.dump(rewardList, open('rewards.p', 'wb'))\n",
    "                        output = \" \"\n",
    "                    break\n",
    "                    #print(sumReward)         \n",
    "            else: \n",
    "                if done:\n",
    "                    output = \"\\n\" \"episode: {}/{}, score: {}, loss: {}\".format(e, EPISODES, agent.total_reward, agent.loss)\n",
    "                    with open(\"test.txt\", \"a\") as myfile:\n",
    "                        myfile.write(output)\n",
    "                    rewardList.append(agent.total_reward)\n",
    "                    pickle.dump(rewardList, open('rewards.p', 'wb'))\n",
    "                    print(output)\n",
    "                    break \n",
    "            \n",
    "        \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
