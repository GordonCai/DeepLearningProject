{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import keras \n",
    "from PIL import Image \n",
    "from collections import deque\n",
    "import random \n",
    "from keras import initializers\n",
    "from keras.initializers import normal, identity\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD , Adam, RMSprop\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import skimage\n",
    "from skimage import color \n",
    "from skimage import color, transform, exposure\n",
    "import argparse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent: \n",
    "    def __init__(self, env): \n",
    "        self.env = env \n",
    "        self.memory = [] \n",
    "        self.gamma = 0.9 \n",
    "        self.epsilon = 1 \n",
    "        self.epsilon_decay = 0.995 \n",
    "        self.epsilon_min = 0.1 \n",
    "        self.learning_rate = 0.0001\n",
    "        self._build_model()\n",
    "    def _build_model(self): \n",
    "        model = Sequential() \n",
    "        model.add(Dense(128, input_dim=4, activation='tanh'))\n",
    "        model.add(Dense(128, activation='tanh'))\n",
    "        model.add(Dense(128, activation='tanh'))\n",
    "        model.add(Dense(2, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=RMSprop(lr=self.learning_rate))\n",
    "        self.model = model\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        batches = min(batch_size, len(self.memory))\n",
    "        batches = np.random.choice(len(self.memory), batches)\n",
    "        for i in batches:\n",
    "            state, action, reward, next_state, done = self.memory[i]\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, nb_epoch=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-19 02:14:10,452] Making new env: CartPole-v0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'copy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-bc6c72def355>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m# make next_state the new current state for the next frame.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# done becomes True when the game ends\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'copy' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  \n",
    "    # initialize gym environment and the agent\n",
    "    env = gym.make('CartPole-v0')\n",
    "    agent = DQNAgent(env)\n",
    "    episodes = 10\n",
    "    # Iterate the game\n",
    "    for e in range(episodes):\n",
    "  \n",
    "        # reset state in the beginning of each game\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, 4])\n",
    "  \n",
    "        # time_t represents each frame of the game\n",
    "        # Our goal is to keep the pole upright as long as possible\n",
    "        # the more time_t the more score\n",
    "        for time_t in range(5000):\n",
    "            # turn this on if you want to render\n",
    "            # env.render()\n",
    "  \n",
    "            # Decide action\n",
    "            action = agent.act(state)\n",
    "  \n",
    "            # Advance the game to the next frame based on the action.\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, 4])\n",
    "  \n",
    "            # (reward defaults to 1)\n",
    "            # reward the agent 1 for every frame it lived\n",
    "            # and punish -100 for dying\n",
    "            reward = -100 if done else reward\n",
    "  \n",
    "            # Remember the previous state, action, reward, and done\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "  \n",
    "            # make next_state the new current state for the next frame.\n",
    "            state = copy.deepcopy(next_state)\n",
    "  \n",
    "            # done becomes True when the game ends\n",
    "            # ex) The agent drops the pole\n",
    "            if done:\n",
    "                # print the score and break out of the loop\n",
    "                print(\"episode: {}/{}, score: {}\"\n",
    "                      .format(e, episodes, time_t))\n",
    "                break\n",
    "        # train the agent with the experience of the episode\n",
    "        agent.replay(32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
